{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nQmg_opc15eIaBXjsS6H8k7c5uQoFYE4",
      "authorship_tag": "ABX9TyM+juT8I5tAhdZYiNZzKgAB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWL-408/CIT2024/blob/main/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E6%8E%A2%E7%B4%A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のハイパーパラメータのすべての組み合わせで供試データ別に誤差を出力する。\n",
        "誤差が最小かつ計算コストが最小となる条件が最適条件である。\n",
        "\n"
      ],
      "metadata": {
        "id": "FQWmd_rk8fYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def create_dataset(data, time_step=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/program_LSTM/input-data'\n",
        "output_folder = '/content/drive/MyDrive/program_LSTM/tuning-result/plot'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# エポックとバッチサイズ、ニューロン数の候補を設定\n",
        "epoch_options = [10, 30, 50, 100]\n",
        "batch_size_options = [32, 64, 128, 256]\n",
        "neuron_options = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "result_summary = []\n",
        "\n",
        "# フォントサイズの設定\n",
        "title_font_size = 24\n",
        "label_font_size = 20\n",
        "legend_font_size = 20\n",
        "ticks_font_size = 18\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 必要な列のみ抽出\n",
        "    required_columns = ['time', 'Nominal strain', 'Nominal stress']\n",
        "    if not all(column in df.columns for column in required_columns):\n",
        "        print(f\"Required columns not found in {filename}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    df = df[required_columns]\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # 時系列データをインデックスに設定\n",
        "    df.set_index('time', inplace=True)\n",
        "\n",
        "    # スケーリング\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # データセット作成\n",
        "    time_step = 10\n",
        "    X, Y = create_dataset(df_scaled, time_step)\n",
        "\n",
        "    # データ分割\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            best_loss = float('inf')\n",
        "            best_neurons = 0\n",
        "\n",
        "            for neurons in neuron_options:\n",
        "                print(f\"Training with {epochs} epochs, batch size of {batch_size}, and {neurons} neurons...\")\n",
        "\n",
        "                # 時間計測開始\n",
        "                start_time = time.time()\n",
        "\n",
        "                # LSTMモデルを構築\n",
        "                model = Sequential()\n",
        "                model.add(LSTM(neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                model.add(LSTM(neurons))\n",
        "                model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "                # モデルの訓練\n",
        "                history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                # 時間計測終了\n",
        "                end_time = time.time()\n",
        "                processing_time = end_time - start_time\n",
        "\n",
        "                # 検証損失を取得\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                print(f\"Validation Loss with {neurons} Neurons: {val_loss:.4f} (Time: {processing_time:.2f} seconds)\")\n",
        "\n",
        "                # 最良のニューロン数を更新\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_neurons = neurons\n",
        "\n",
        "                # 学習履歴データを保存\n",
        "                history_df = pd.DataFrame(history.history)\n",
        "                history_filename = f\"{filename}_epochs-{epochs}_batch-{batch_size}_neurons-{neurons}_history.csv\"\n",
        "                history_df.to_csv(os.path.join(output_folder, history_filename), index=False)\n",
        "\n",
        "                # 学習曲線をプロット\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(history.history['loss'], label='Train Loss')\n",
        "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "                # タイトル、軸ラベル、凡例の設定\n",
        "                plt.title(f'Model Loss for {filename}\\nEpochs: {epochs}, Batch Size: {batch_size}, Neurons: {neurons}',\n",
        "                          fontsize=title_font_size, fontweight='bold')\n",
        "                plt.ylabel('Loss', fontsize=label_font_size)\n",
        "                plt.xlabel('Epoch', fontsize=label_font_size)\n",
        "                plt.legend(loc='upper right', fontsize=legend_font_size)\n",
        "                plt.xticks(fontsize=ticks_font_size)\n",
        "                plt.yticks(fontsize=ticks_font_size)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{output_folder}/{filename}_epochs-{epochs}_batch-{batch_size}_neurons-{neurons}_plot.png\", dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                # 条件別の結果を保存\n",
        "                result_summary.append({\n",
        "                    'filename': filename,\n",
        "                    'epochs': epochs,\n",
        "                    'batch_size': batch_size,\n",
        "                    'neurons': neurons,\n",
        "                    'val_loss': val_loss,\n",
        "                    'processing_time': processing_time\n",
        "                })\n",
        "\n",
        "# 最終的な結果をCSVファイルに保存\n",
        "results_df = pd.DataFrame(result_summary)\n",
        "results_summary_path = '/content/drive/MyDrive/program_LSTM/tuning-result/LSTM_tuning_summary.csv'\n",
        "results_df.to_csv(results_summary_path, index=False)\n",
        "\n",
        "print(f'Tuning results saved to {results_summary_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD4YTDW2dpTj",
        "outputId": "fa895100-c3bb-4b6f-ca67-94c7a685eaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training with 10 epochs, batch size of 32, and 1 neurons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss with 1 Neurons: 0.0169 (Time: 5.60 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0370 (Time: 5.58 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0079 (Time: 5.69 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0107 (Time: 5.63 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0101 (Time: 6.34 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0136 (Time: 7.14 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0096 (Time: 5.47 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0097 (Time: 7.31 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0079 (Time: 5.87 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0085 (Time: 5.51 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0086 (Time: 6.34 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0068 (Time: 7.57 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.6706 (Time: 4.45 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0200 (Time: 5.26 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0240 (Time: 5.04 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0069 (Time: 5.33 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0211 (Time: 4.74 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0186 (Time: 6.02 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0160 (Time: 5.52 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0136 (Time: 4.67 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0279 (Time: 4.95 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0164 (Time: 5.48 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0117 (Time: 6.55 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0116 (Time: 7.93 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.6546 (Time: 5.21 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0169 (Time: 4.97 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0589 (Time: 4.55 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0504 (Time: 4.73 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0184 (Time: 4.88 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0301 (Time: 4.78 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0175 (Time: 4.09 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0235 (Time: 7.18 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0227 (Time: 6.59 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0186 (Time: 5.45 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0114 (Time: 8.36 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0169 (Time: 5.66 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.7767 (Time: 5.73 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0721 (Time: 4.89 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.1042 (Time: 4.47 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0193 (Time: 4.63 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0181 (Time: 4.31 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0155 (Time: 5.94 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0149 (Time: 5.35 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0149 (Time: 4.79 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0496 (Time: 6.34 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0232 (Time: 5.02 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0443 (Time: 6.93 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0664 (Time: 6.48 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0257 (Time: 11.22 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0204 (Time: 11.46 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0055 (Time: 11.92 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0127 (Time: 12.26 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0085 (Time: 15.61 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0082 (Time: 12.86 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0099 (Time: 11.90 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0079 (Time: 12.45 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0077 (Time: 12.61 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0079 (Time: 13.44 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0081 (Time: 14.53 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0084 (Time: 15.67 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.1105 (Time: 6.85 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0130 (Time: 8.95 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0083 (Time: 9.34 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0082 (Time: 9.52 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0091 (Time: 8.45 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0099 (Time: 13.76 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0087 (Time: 11.68 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0078 (Time: 12.23 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0090 (Time: 12.17 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0073 (Time: 12.80 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0071 (Time: 13.15 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0079 (Time: 13.65 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0948 (Time: 8.20 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0294 (Time: 8.19 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0061 (Time: 7.42 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0133 (Time: 7.41 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0160 (Time: 7.68 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0158 (Time: 7.43 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0092 (Time: 8.59 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0102 (Time: 10.45 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0089 (Time: 11.35 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0100 (Time: 16.44 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0089 (Time: 13.63 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0085 (Time: 13.90 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0205 (Time: 6.23 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0262 (Time: 8.00 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0285 (Time: 7.74 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0297 (Time: 6.94 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0150 (Time: 7.18 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0279 (Time: 8.67 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0242 (Time: 9.90 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0298 (Time: 9.74 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0177 (Time: 9.88 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0189 (Time: 11.16 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0165 (Time: 11.98 seconds)\n",
            "Training with 30 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0200 (Time: 11.56 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.2377 (Time: 17.94 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0270 (Time: 15.13 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0096 (Time: 16.76 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0134 (Time: 17.36 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0099 (Time: 16.68 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0078 (Time: 25.79 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0084 (Time: 24.38 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0078 (Time: 24.61 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0080 (Time: 24.35 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0079 (Time: 24.89 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0078 (Time: 25.25 seconds)\n",
            "Training with 50 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0080 (Time: 23.40 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0312 (Time: 11.25 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0360 (Time: 11.63 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0101 (Time: 13.28 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0147 (Time: 12.76 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0082 (Time: 13.35 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0083 (Time: 14.58 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0069 (Time: 13.58 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0081 (Time: 14.07 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0081 (Time: 17.00 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0088 (Time: 19.55 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0082 (Time: 20.17 seconds)\n",
            "Training with 50 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0078 (Time: 22.45 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0255 (Time: 10.67 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0249 (Time: 9.69 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0131 (Time: 12.07 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0128 (Time: 10.55 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0064 (Time: 17.81 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0083 (Time: 13.42 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0094 (Time: 14.49 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0076 (Time: 16.91 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0086 (Time: 20.87 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0086 (Time: 19.07 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0094 (Time: 19.77 seconds)\n",
            "Training with 50 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0075 (Time: 20.62 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.5535 (Time: 9.97 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0184 (Time: 9.99 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0419 (Time: 9.03 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0412 (Time: 9.83 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0346 (Time: 10.29 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0235 (Time: 11.90 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0223 (Time: 11.39 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0131 (Time: 17.01 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0091 (Time: 13.79 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0121 (Time: 15.19 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0095 (Time: 16.98 seconds)\n",
            "Training with 50 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0080 (Time: 18.18 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0413 (Time: 30.65 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0051 (Time: 30.33 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0078 (Time: 31.87 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0135 (Time: 32.53 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0081 (Time: 31.43 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0075 (Time: 33.44 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0078 (Time: 34.21 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0080 (Time: 60.08 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0078 (Time: 49.49 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0084 (Time: 54.70 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0080 (Time: 58.35 seconds)\n",
            "Training with 100 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0084 (Time: 55.59 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0388 (Time: 21.86 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0194 (Time: 24.60 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0062 (Time: 22.12 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0138 (Time: 22.87 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0089 (Time: 24.00 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0088 (Time: 23.68 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0089 (Time: 26.26 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0091 (Time: 25.64 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0078 (Time: 31.51 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0093 (Time: 38.08 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0077 (Time: 39.31 seconds)\n",
            "Training with 100 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0084 (Time: 39.76 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0254 (Time: 18.02 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0242 (Time: 17.80 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0087 (Time: 19.16 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0128 (Time: 18.43 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0107 (Time: 19.60 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0097 (Time: 20.92 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0086 (Time: 21.41 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0092 (Time: 28.45 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0081 (Time: 31.61 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0072 (Time: 31.94 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0080 (Time: 33.88 seconds)\n",
            "Training with 100 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0081 (Time: 35.24 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0211 (Time: 13.40 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0264 (Time: 16.48 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0261 (Time: 15.76 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0145 (Time: 16.83 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0156 (Time: 18.12 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0087 (Time: 32.84 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0080 (Time: 27.08 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0087 (Time: 33.69 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0082 (Time: 35.17 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0078 (Time: 35.79 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0080 (Time: 37.43 seconds)\n",
            "Training with 100 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0078 (Time: 37.07 seconds)\n",
            "Tuning results saved to /content/drive/MyDrive/program_LSTM/tuning-result/LSTM_tuning_M_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "サポートコードA：全データでは処理が重くなるためデータ個数指定しリサンプリングする場合"
      ],
      "metadata": {
        "id": "jl1Rpqeh7MmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def create_dataset(data, time_step=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/program_LSTM/input-data(2)/F'\n",
        "output_folder = '/content/drive/MyDrive/program_LSTM/tuning-result_resampling1000/plot'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# エポックとバッチサイズ、ニューロン数の候補を設定\n",
        "epoch_options = [10, 30, 50, 100]\n",
        "batch_size_options = [32, 64, 128, 256]\n",
        "neuron_options = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "result_summary = []\n",
        "\n",
        "# フォントサイズの設定\n",
        "title_font_size = 24\n",
        "label_font_size = 20\n",
        "legend_font_size = 20\n",
        "ticks_font_size = 18\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 必要な列のみ抽出\n",
        "    required_columns = ['time', 'Nominal strain', 'Nominal stress']\n",
        "    if not all(column in df.columns for column in required_columns):\n",
        "        print(f\"Required columns not found in {filename}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    df = df[required_columns]\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # 時系列データをインデックスに設定\n",
        "    df.set_index('time', inplace=True)\n",
        "\n",
        "    # データクリーニング: 空欄行を削除し、重複を削除\n",
        "    df.dropna(inplace=True)\n",
        "    df = df[~df.index.duplicated(keep='first')]  # 重複するインデックスを削除\n",
        "\n",
        "    # リサンプリングでデータ点数を1000点に揃える\n",
        "    index_new = pd.date_range(start=df.index.min(), end=df.index.max(), periods=1000)\n",
        "    df_resampled = df.reindex(index_new).interpolate(method='linear')\n",
        "\n",
        "    # スケーリング\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = scaler.fit_transform(df_resampled)\n",
        "\n",
        "    # データセット作成\n",
        "    time_step = 10\n",
        "    X, Y = create_dataset(df_scaled, time_step)\n",
        "\n",
        "    # データ分割\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "    # モデルのトレーニング\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(f\"Not enough data in {filename} for training and testing. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            best_loss = float('inf')\n",
        "            best_neurons = 0\n",
        "\n",
        "            for neurons in neuron_options:\n",
        "                print(f\"Training with {epochs} epochs, batch size of {batch_size}, and {neurons} neurons...\")\n",
        "\n",
        "                # 時間計測開始\n",
        "                start_time = time.time()\n",
        "\n",
        "                # LSTMモデルを構築\n",
        "                model = Sequential()\n",
        "                model.add(LSTM(neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                model.add(LSTM(neurons))\n",
        "                model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "                # モデルの訓練\n",
        "                history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                # 時間計測終了\n",
        "                end_time = time.time()\n",
        "                processing_time = end_time - start_time\n",
        "\n",
        "                # 検証損失を取得\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                print(f\"Validation Loss with {neurons} Neurons: {val_loss:.4f} (Time: {processing_time:.2f} seconds)\")\n",
        "\n",
        "                # 最良のニューロン数を更新\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_neurons = neurons\n",
        "\n",
        "                # 学習履歴データを保存\n",
        "                history_df = pd.DataFrame(history.history)\n",
        "                history_filename = f\"{filename}_epochs-{epochs}_batch-{batch_size}_neurons-{neurons}_history.csv\"\n",
        "                history_df.to_csv(os.path.join(output_folder, history_filename), index=False)\n",
        "\n",
        "                # 学習曲線をプロット\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(history.history['loss'], label='Train Loss')\n",
        "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "                # タイトル、軸ラベル、凡例の設定\n",
        "                plt.title(f'Model Loss for {filename}\\nEpochs: {epochs}, Batch Size: {batch_size}, Neurons: {neurons}',\n",
        "                          fontsize=title_font_size, fontweight='bold')\n",
        "                plt.ylabel('Loss', fontsize=label_font_size)\n",
        "                plt.xlabel('Epoch', fontsize=label_font_size)\n",
        "                plt.legend(loc='upper right', fontsize=legend_font_size)\n",
        "                plt.xticks(fontsize=ticks_font_size)\n",
        "                plt.yticks(fontsize=ticks_font_size)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{output_folder}/{filename}_epochs-{epochs}_batch-{batch_size}_neurons-{neurons}_plot.png\", dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                # 条件別の結果を保存\n",
        "                result_summary.append({\n",
        "                    'filename': filename,\n",
        "                    'epochs': epochs,\n",
        "                    'batch_size': batch_size,\n",
        "                    'neurons': neurons,\n",
        "                    'val_loss': val_loss,\n",
        "                    'processing_time': processing_time\n",
        "                })\n",
        "\n",
        "# 最終的な結果をCSVファイルに保存\n",
        "results_df = pd.DataFrame(result_summary)\n",
        "results_summary_path = '/content/drive/MyDrive/program_LSTM/tuning-result/LSTM_tuning_F1000_summary.csv'\n",
        "results_df.to_csv(results_summary_path, index=False)\n",
        "\n",
        "print(f'Tuning results saved to {results_summary_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n2Lv7PjJ4Nk",
        "outputId": "a02c844f-333a-4b08-c421-f23900f1f1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training with 10 epochs, batch size of 32, and 1 neurons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss with 1 Neurons: 0.0162 (Time: 7.72 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0099 (Time: 6.56 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0023 (Time: 6.39 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0003 (Time: 8.72 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0011 (Time: 6.11 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0004 (Time: 7.57 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0006 (Time: 11.13 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0003 (Time: 8.68 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0003 (Time: 8.60 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0002 (Time: 6.55 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0000 (Time: 8.61 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0002 (Time: 7.46 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.1778 (Time: 6.25 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.1174 (Time: 7.04 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0293 (Time: 5.19 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0024 (Time: 6.36 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0004 (Time: 6.38 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0008 (Time: 5.45 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0004 (Time: 6.75 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0002 (Time: 6.46 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0001 (Time: 6.17 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0001 (Time: 7.18 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0000 (Time: 8.66 seconds)\n",
            "Training with 10 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0001 (Time: 8.83 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0189 (Time: 4.07 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0016 (Time: 7.24 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.1395 (Time: 4.93 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0046 (Time: 5.34 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0007 (Time: 6.11 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0001 (Time: 5.11 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0012 (Time: 5.89 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0001 (Time: 25.37 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0002 (Time: 7.68 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0001 (Time: 8.88 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0005 (Time: 8.66 seconds)\n",
            "Training with 10 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0002 (Time: 11.28 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0789 (Time: 4.53 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0052 (Time: 6.89 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0082 (Time: 5.79 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.1790 (Time: 5.15 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0139 (Time: 5.80 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0006 (Time: 8.39 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0017 (Time: 5.80 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0060 (Time: 7.06 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0010 (Time: 7.94 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0001 (Time: 7.71 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0000 (Time: 6.20 seconds)\n",
            "Training with 10 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0000 (Time: 8.28 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0035 (Time: 13.08 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0128 (Time: 11.08 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0041 (Time: 11.16 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0025 (Time: 11.70 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0010 (Time: 11.64 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0012 (Time: 11.71 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0005 (Time: 11.84 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0005 (Time: 13.11 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0002 (Time: 12.93 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0003 (Time: 13.13 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0001 (Time: 15.36 seconds)\n",
            "Training with 30 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0001 (Time: 15.07 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.1372 (Time: 7.94 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0159 (Time: 8.06 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0028 (Time: 7.82 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0016 (Time: 9.04 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0013 (Time: 9.21 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0009 (Time: 9.50 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0003 (Time: 9.78 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0004 (Time: 9.38 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0003 (Time: 10.99 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0002 (Time: 11.87 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0001 (Time: 12.63 seconds)\n",
            "Training with 30 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0001 (Time: 13.74 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.1744 (Time: 6.98 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0063 (Time: 7.36 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0006 (Time: 9.56 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0021 (Time: 8.41 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0002 (Time: 7.67 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0004 (Time: 7.69 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0009 (Time: 7.74 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0004 (Time: 10.15 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0003 (Time: 11.59 seconds)\n",
            "Training with 30 epochs, batch size of 128, and 80 neurons...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "サポートコードB：ハイパーパラメータ検証プログラムが途中停止の場合のテキスト出力からcsvに変換するためのプログラム"
      ],
      "metadata": {
        "id": "ZM5qKWDIVbay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 入力ファイルと出力ファイルのパスを設定\n",
        "input_file_path = '/content/drive/MyDrive/program_LSTM/tuning-result/hyperparameter_result_B.txt'\n",
        "output_file_path = '/content/drive/MyDrive/program_LSTM/tuning-result/hyperparameter_results_B01.csv'\n",
        "\n",
        "# txtファイルの内容を読み込む\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# デバッグ用：読み込んだ内容の一部を表示\n",
        "print(\"ファイル内容（一部）:\\n\", text_data[:500])\n",
        "\n",
        "# 正規表現パターンの作成\n",
        "# ここでは、2行分の結果（Training ... と Validation Loss ...）を1組として抽出します。\n",
        "pattern = re.compile(\n",
        "    r\"Training with (\\d+)\\s+epochs,\\s+batch size of (\\d+),\\s+and (\\d+)\\s+neurons\\.\\.\\.\\s*\"\n",
        "    r\"Validation Loss with \\d+\\s+Neurons:\\s+([\\d\\.]+)\\s+\\(Time:\\s+([\\d\\.]+)\\s+seconds\\)\",\n",
        "    re.MULTILINE\n",
        ")\n",
        "\n",
        "# 正規表現を用いて該当データを抽出する\n",
        "matches = pattern.findall(text_data)\n",
        "print(\"抽出されたエントリ数:\", len(matches))\n",
        "print(\"抽出内容（デバッグ用）:\", matches)\n",
        "\n",
        "# ファイル名（任意で設定）を変数に格納\n",
        "filename = 'B.csv'\n",
        "\n",
        "# pandasのDataFrameに変換する\n",
        "# 抽出されたmatchesは各タプル形式で、要素は (epochs, batch_size, neurons, val_loss, processing_time)\n",
        "df = pd.DataFrame(matches, columns=['epochs', 'batch_size', 'neurons', 'val_loss', 'processing_time'])\n",
        "\n",
        "# データ型の変換：整数型や浮動小数点型へ\n",
        "df['epochs'] = df['epochs'].astype(int)\n",
        "df['batch_size'] = df['batch_size'].astype(int)\n",
        "df['neurons'] = df['neurons'].astype(int)\n",
        "df['val_loss'] = df['val_loss'].astype(float)\n",
        "df['processing_time'] = df['processing_time'].astype(float)\n",
        "\n",
        "# DataFrameの先頭にfilename列を追加する\n",
        "df.insert(0, 'filename', filename)\n",
        "\n",
        "# 結果をCSVファイルに保存する\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"結果がCSVファイルに保存されました: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UWmR6Bla32d",
        "outputId": "6287bcb2-44d3-4ce9-eec3-7ef66f5172dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ファイル内容（一部）:\n",
            " Training with 10 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.0706 (Time: 10.67 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.0056 (Time: 19.47 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0152 (Time: 13.85 seconds)\n",
            "Training with 10 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0207 (Time: 13.27 seconds)\n",
            "Training wit\n",
            "抽出されたエントリ数: 191\n",
            "抽出内容（デバッグ用）: [('10', '32', '1', '0.0706', '10.67'), ('10', '32', '5', '0.0056', '19.47'), ('10', '32', '10', '0.0152', '13.85'), ('10', '32', '20', '0.0207', '13.27'), ('10', '32', '30', '0.0056', '14.36'), ('10', '32', '40', '0.0072', '15.65'), ('10', '32', '50', '0.0034', '14.41'), ('10', '32', '60', '0.0016', '15.41'), ('10', '32', '70', '0.0018', '17.94'), ('10', '32', '80', '0.0018', '16.77'), ('10', '32', '90', '0.0023', '19.58'), ('10', '32', '100', '0.0022', '20.93'), ('10', '64', '1', '0.0660', '6.55'), ('10', '64', '5', '0.0702', '8.49'), ('10', '64', '10', '0.0758', '8.60'), ('10', '64', '20', '0.0229', '9.50'), ('10', '64', '30', '0.0203', '8.61'), ('10', '64', '40', '0.0043', '9.20'), ('10', '64', '50', '0.0058', '10.35'), ('10', '64', '60', '0.0041', '9.83'), ('10', '64', '70', '0.0044', '12.10'), ('10', '64', '80', '0.0031', '17.41'), ('10', '64', '90', '0.0028', '15.85'), ('10', '64', '100', '0.0029', '17.32'), ('10', '128', '1', '0.3152', '5.70'), ('10', '128', '5', '0.0218', '5.82'), ('10', '128', '10', '0.0263', '5.19'), ('10', '128', '20', '0.0148', '6.65'), ('10', '128', '30', '0.0205', '5.78'), ('10', '128', '40', '0.0136', '7.46'), ('10', '128', '50', '0.0120', '8.94'), ('10', '128', '60', '0.0077', '9.15'), ('10', '128', '70', '0.0118', '11.83'), ('10', '128', '80', '0.0075', '13.12'), ('10', '128', '90', '0.0088', '14.42'), ('10', '128', '100', '0.0068', '14.25'), ('10', '256', '1', '0.1911', '4.83'), ('10', '256', '5', '0.0395', '4.83'), ('10', '256', '10', '0.0146', '4.99'), ('10', '256', '20', '0.0372', '5.02'), ('10', '256', '30', '0.0108', '5.48'), ('10', '256', '40', '0.0229', '6.00'), ('10', '256', '50', '0.0118', '8.22'), ('10', '256', '60', '0.0148', '9.84'), ('10', '256', '70', '0.0107', '8.86'), ('10', '256', '80', '0.0109', '8.47'), ('10', '256', '90', '0.0094', '11.21'), ('10', '256', '100', '0.0100', '13.88'), ('30', '32', '1', '0.0167', '27.13'), ('30', '32', '5', '0.0076', '41.87'), ('30', '32', '10', '0.0033', '35.60'), ('30', '32', '20', '0.0003', '34.33'), ('30', '32', '30', '0.0034', '38.78'), ('30', '32', '40', '0.0020', '38.70'), ('30', '32', '50', '0.0005', '40.67'), ('30', '32', '60', '0.0019', '39.90'), ('30', '32', '70', '0.0008', '46.22'), ('30', '32', '80', '0.0008', '55.32'), ('30', '32', '90', '0.0007', '53.64'), ('30', '32', '100', '0.0007', '64.17'), ('30', '64', '1', '0.0445', '18.98'), ('30', '64', '5', '0.0293', '18.76'), ('30', '64', '10', '0.0026', '19.72'), ('30', '64', '20', '0.0050', '20.87'), ('30', '64', '30', '0.0018', '21.52'), ('30', '64', '40', '0.0019', '23.41'), ('30', '64', '50', '0.0024', '23.98'), ('30', '64', '60', '0.0024', '24.82'), ('30', '64', '70', '0.0017', '33.88'), ('30', '64', '80', '0.0014', '41.47'), ('30', '64', '90', '0.0018', '41.54'), ('30', '64', '100', '0.0024', '47.33'), ('30', '128', '1', '0.0489', '11.51'), ('30', '128', '5', '0.0684', '11.05'), ('30', '128', '10', '0.0258', '12.16'), ('30', '128', '20', '0.0020', '12.15'), ('30', '128', '30', '0.0040', '12.52'), ('30', '128', '40', '0.0035', '15.10'), ('30', '128', '50', '0.0058', '19.09'), ('30', '128', '60', '0.0029', '26.50'), ('30', '128', '70', '0.0031', '27.84'), ('30', '128', '80', '0.0028', '33.17'), ('30', '128', '90', '0.0024', '36.73'), ('30', '128', '100', '0.0029', '40.33'), ('30', '256', '1', '0.0585', '8.73'), ('30', '256', '5', '0.0016', '9.29'), ('30', '256', '10', '0.0094', '10.06'), ('30', '256', '20', '0.0187', '10.54'), ('30', '256', '30', '0.0120', '11.65'), ('30', '256', '40', '0.0104', '17.54'), ('30', '256', '50', '0.0064', '19.48'), ('30', '256', '60', '0.0071', '20.96'), ('30', '256', '70', '0.0061', '22.87'), ('30', '256', '80', '0.0070', '23.12'), ('30', '256', '90', '0.0071', '30.17'), ('30', '256', '100', '0.0058', '35.77'), ('50', '32', '1', '0.0599', '48.14'), ('50', '32', '5', '0.0058', '51.55'), ('50', '32', '10', '0.0005', '55.41'), ('50', '32', '20', '0.0024', '55.38'), ('50', '32', '30', '0.0014', '59.20'), ('50', '32', '40', '0.0011', '59.74'), ('50', '32', '50', '0.0011', '60.45'), ('50', '32', '60', '0.0016', '68.77'), ('50', '32', '70', '0.0005', '71.85'), ('50', '32', '80', '0.0016', '74.57'), ('50', '32', '90', '0.0003', '97.25'), ('50', '32', '100', '0.0003', '113.49'), ('50', '64', '1', '0.0424', '29.43'), ('50', '64', '5', '0.0484', '31.14'), ('50', '64', '10', '0.0016', '30.73'), ('50', '64', '20', '0.0129', '33.18'), ('50', '64', '30', '0.0007', '34.52'), ('50', '64', '40', '0.0024', '37.40'), ('50', '64', '50', '0.0019', '42.44'), ('50', '64', '60', '0.0031', '41.28'), ('50', '64', '70', '0.0012', '52.88'), ('50', '64', '80', '0.0021', '67.37'), ('50', '64', '90', '0.0014', '78.56'), ('50', '64', '100', '0.0010', '90.83'), ('50', '128', '1', '0.0630', '17.72'), ('50', '128', '5', '0.0460', '18.37'), ('50', '128', '10', '0.0131', '18.64'), ('50', '128', '20', '0.0079', '20.42'), ('50', '128', '30', '0.0091', '23.02'), ('50', '128', '40', '0.0011', '25.32'), ('50', '128', '50', '0.0040', '28.54'), ('50', '128', '60', '0.0028', '42.09'), ('50', '128', '70', '0.0021', '50.05'), ('50', '128', '80', '0.0016', '53.26'), ('50', '128', '90', '0.0017', '60.30'), ('50', '128', '100', '0.0021', '62.88'), ('50', '256', '1', '0.0816', '12.02'), ('50', '256', '5', '0.0009', '14.83'), ('50', '256', '10', '0.0128', '14.94'), ('50', '256', '20', '0.0062', '15.98'), ('50', '256', '30', '0.0047', '16.62'), ('50', '256', '40', '0.0051', '25.30'), ('50', '256', '50', '0.0043', '30.71'), ('50', '256', '60', '0.0029', '32.23'), ('50', '256', '70', '0.0034', '35.05'), ('50', '256', '80', '0.0027', '37.24'), ('50', '256', '90', '0.0022', '52.02'), ('50', '256', '100', '0.0033', '56.39'), ('100', '32', '1', '0.0055', '94.68'), ('100', '32', '5', '0.0002', '101.77'), ('100', '32', '10', '0.0005', '109.21'), ('100', '32', '20', '0.0002', '106.17'), ('100', '32', '30', '0.0001', '112.30'), ('100', '32', '40', '0.0006', '118.46'), ('100', '32', '50', '0.0002', '130.70'), ('100', '32', '60', '0.0003', '128.08'), ('100', '32', '70', '0.0002', '142.85'), ('100', '32', '80', '0.0013', '148.53'), ('100', '32', '90', '0.0002', '167.46'), ('100', '32', '100', '0.0003', '205.91'), ('100', '64', '5', '0.0116', '23.85'), ('100', '64', '10', '0.0108', '25.62'), ('100', '64', '20', '0.0076', '29.35'), ('100', '64', '30', '0.0079', '29.93'), ('100', '64', '40', '0.0089', '29.50'), ('100', '64', '50', '0.0061', '35.14'), ('100', '64', '60', '0.0060', '35.23'), ('100', '64', '70', '0.0054', '36.46'), ('100', '64', '80', '0.0056', '60.51'), ('100', '64', '90', '0.0066', '64.99'), ('100', '64', '100', '0.0059', '70.28'), ('100', '128', '1', '0.0770', '19.19'), ('100', '128', '5', '0.0552', '23.09'), ('100', '128', '10', '0.0289', '22.06'), ('100', '128', '20', '0.0070', '24.25'), ('100', '128', '30', '0.0078', '23.79'), ('100', '128', '40', '0.0074', '27.17'), ('100', '128', '50', '0.0067', '30.95'), ('100', '128', '60', '0.0068', '40.64'), ('100', '128', '70', '0.0071', '37.79'), ('100', '128', '80', '0.0064', '41.85'), ('100', '128', '90', '0.0059', '48.58'), ('100', '128', '100', '0.0066', '58.63'), ('100', '256', '1', '0.0196', '16.39'), ('100', '256', '5', '0.0067', '18.05'), ('100', '256', '10', '0.0119', '18.76'), ('100', '256', '20', '0.0089', '19.68'), ('100', '256', '30', '0.0101', '20.97'), ('100', '256', '40', '0.0117', '27.54'), ('100', '256', '50', '0.0068', '31.23'), ('100', '256', '60', '0.0052', '35.06'), ('100', '256', '70', '0.0068', '36.35'), ('100', '256', '80', '0.0065', '58.27'), ('100', '256', '90', '0.0067', '52.78'), ('100', '256', '100', '0.0065', '63.31')]\n",
            "結果がCSVファイルに保存されました: /content/drive/MyDrive/program_LSTM/tuning-result/hyperparameter_results_B01.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UGzGNo1m0BtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def create_dataset(data, time_step=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def save_intermediate_results(filepath, results):\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "def load_intermediate_results(filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/program_LSTM/input-data'\n",
        "intermediate_results_file = '/content/drive/MyDrive/program_LSTM/tuning_result.json'\n",
        "\n",
        "# エポックとバッチサイズ、ニューロン数の候補を設定\n",
        "epoch_options = [10, 30, 50, 100]\n",
        "batch_size_options = [32, 64, 128]\n",
        "neuron_options = [40, 50, 60, 70, 80]\n",
        "\n",
        "# 既存の途中結果を読み込む\n",
        "intermediate_results = load_intermediate_results(intermediate_results_file)\n",
        "\n",
        "# 全データセットのグローバルな最小損失とそのパラメータを記録\n",
        "global_best_loss = float('inf')\n",
        "global_best_params = {}\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 必要な列のみ抽出\n",
        "    required_columns = ['time', 'Nominal strain', 'Nominal stress']\n",
        "    if not all(column in df.columns for column in required_columns):\n",
        "        print(f\"Required columns not found in {filename}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    df = df[required_columns]\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # レスポンスタイムを1秒ごとにリサンプリングして100点にする\n",
        "    df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
        "\n",
        "    # スケーリング\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # データセット作成\n",
        "    time_step = 10\n",
        "    X, Y = create_dataset(df_scaled, time_step)\n",
        "\n",
        "    # データ分割\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "    best_loss_overall = float('inf')\n",
        "    best_params_overall = {}\n",
        "\n",
        "    mse_values = []  # 各組み合わせのMSEを格納\n",
        "\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            for neurons in neuron_options:\n",
        "                # 中断したところから再開するために結果をチェック\n",
        "                result_key = f\"{filename}_{epochs}_{batch_size}_{neurons}\"\n",
        "                if result_key in intermediate_results:\n",
        "                    val_loss = intermediate_results[result_key]\n",
        "                else:\n",
        "                    print(f\"Training with {epochs} epochs, batch size of {batch_size}, and {neurons} neurons...\")\n",
        "\n",
        "                    # LSTMモデルを構築\n",
        "                    model = Sequential()\n",
        "                    model.add(LSTM(neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                    model.add(LSTM(neurons))\n",
        "                    model.add(Dense(1))\n",
        "                    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "                    # モデルの訓練\n",
        "                    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                        epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                    # 検証損失を取得\n",
        "                    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "                    # 中間結果を保存\n",
        "                    intermediate_results[result_key] = val_loss\n",
        "                    save_intermediate_results(intermediate_results_file, intermediate_results)\n",
        "\n",
        "                mse_values.append(val_loss)\n",
        "\n",
        "    # 現在のデータセットでの最小のMSEを計算\n",
        "    min_mse = min(mse_values)\n",
        "\n",
        "    # 補正された検証損失で評価\n",
        "    for key, val_loss in intermediate_results.items():\n",
        "        if key.startswith(filename):\n",
        "            corrected_val_loss = val_loss - min_mse\n",
        "\n",
        "            if corrected_val_loss < best_loss_overall:\n",
        "                best_loss_overall = corrected_val_loss\n",
        "                best_params_overall = {\n",
        "                    'params_key': key,\n",
        "                    'corrected_val_loss': corrected_val_loss\n",
        "                }\n",
        "\n",
        "    # 現在のデータセットでの最良のハイパーパラメータを出力\n",
        "    print(f\"Best Parameters for {filename}: {best_params_overall['params_key']}, \"\n",
        "          f\"Corrected Validation Loss = {best_loss_overall:.4f}\")\n",
        "\n",
        "    # 全体での最良のハイパーパラメータを更新\n",
        "    if best_loss_overall < global_best_loss:\n",
        "        global_best_loss = best_loss_overall\n",
        "        global_best_params = {\n",
        "            'filename': filename,\n",
        "            'params_key': best_params_overall['params_key'],\n",
        "            'corrected_val_loss': best_loss_overall\n",
        "        }\n",
        "\n",
        "# 全データセットに対する最良のハイパーパラメータを出力\n",
        "print(f\"\\nGlobal Best Parameters Across All Datasets: Filename = {global_best_params['filename']}, \"\n",
        "      f\"Params Key = {global_best_params['params_key']}, \"\n",
        "      f\"Corrected Validation Loss = {global_best_params['corrected_val_loss']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC4tqlDzcyau",
        "outputId": "bf5e0eb4-422e-40c6-9022-18cc638d916b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Best Parameters for B.csv: B.csv_100_128_60, Corrected Validation Loss = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-614432f4b722>:64: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
            "<ipython-input-1-614432f4b722>:64: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for F-P.csv: F-P.csv_50_64_60, Corrected Validation Loss = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-614432f4b722>:64: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for F.csv: F.csv_50_32_40, Corrected Validation Loss = 0.0000\n",
            "Training with 50 epochs, batch size of 128, and 40 neurons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-614432f4b722>:64: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 50 epochs, batch size of 128, and 50 neurons...\n",
            "Training with 50 epochs, batch size of 128, and 60 neurons...\n",
            "Training with 50 epochs, batch size of 128, and 70 neurons...\n",
            "Training with 50 epochs, batch size of 128, and 80 neurons...\n",
            "Training with 100 epochs, batch size of 32, and 40 neurons...\n",
            "Training with 100 epochs, batch size of 32, and 50 neurons...\n",
            "Training with 100 epochs, batch size of 32, and 60 neurons...\n",
            "Training with 100 epochs, batch size of 32, and 70 neurons...\n",
            "Training with 100 epochs, batch size of 32, and 80 neurons...\n",
            "Training with 100 epochs, batch size of 64, and 40 neurons...\n",
            "Training with 100 epochs, batch size of 64, and 50 neurons...\n",
            "Training with 100 epochs, batch size of 64, and 60 neurons...\n",
            "Training with 100 epochs, batch size of 64, and 70 neurons...\n",
            "Training with 100 epochs, batch size of 64, and 80 neurons...\n",
            "Training with 100 epochs, batch size of 128, and 40 neurons...\n",
            "Training with 100 epochs, batch size of 128, and 50 neurons...\n",
            "Training with 100 epochs, batch size of 128, and 60 neurons...\n",
            "Training with 100 epochs, batch size of 128, and 70 neurons...\n",
            "Training with 100 epochs, batch size of 128, and 80 neurons...\n",
            "Best Parameters for M.csv: M.csv_50_128_70, Corrected Validation Loss = 0.0000\n",
            "\n",
            "Global Best Parameters Across All Datasets: Filename = B.csv, Params Key = B.csv_100_128_60, Corrected Validation Loss = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def create_dataset(data, time_step=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/program_LSTM/input-data'\n",
        "\n",
        "# エポックとバッチサイズ、ニューロン数の候補を設定\n",
        "epoch_options = [10, 30, 50, 100]\n",
        "batch_size_options = [32, 64, 128, 256]\n",
        "neuron_options = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "# 全データセットのグローバルな最小損失とそのパラメータを記録\n",
        "global_best_loss = float('inf')\n",
        "global_best_params = {}\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 必要な列のみ抽出\n",
        "    required_columns = ['time', 'Nominal strain', 'Nominal stress']\n",
        "    if not all(column in df.columns for column in required_columns):\n",
        "        print(f\"Required columns not found in {filename}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    df = df[required_columns]\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # レスポンスタイムを1秒ごとにリサンプリングして100点にする\n",
        "    df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
        "\n",
        "    # スケーリング\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # データセット作成\n",
        "    time_step = 10\n",
        "    X, Y = create_dataset(df_scaled, time_step)\n",
        "\n",
        "    # データ分割\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "    best_loss_overall = float('inf')\n",
        "    best_params_overall = {}\n",
        "\n",
        "    mse_values = []  # 各組み合わせのMSEを格納\n",
        "\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            for neurons in neuron_options:\n",
        "                print(f\"Training with {epochs} epochs, batch size of {batch_size}, and {neurons} neurons...\")\n",
        "\n",
        "                # LSTMモデルを構築\n",
        "                model = Sequential()\n",
        "                model.add(LSTM(neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                model.add(LSTM(neurons))\n",
        "                model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "                # モデルの訓練\n",
        "                history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                # 検証損失を取得\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                mse_values.append(val_loss)\n",
        "\n",
        "    # 現在のデータセットでの最小のMSEを計算\n",
        "    min_mse = min(mse_values)\n",
        "\n",
        "    # 再度ループして補正されたval_lossを使って評価\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            for neurons in neuron_options:\n",
        "                # モデルの訓練\n",
        "                history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                # 検証損失と補正済み評価\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                corrected_val_loss = val_loss - min_mse\n",
        "                print(f\"Corrected Val Loss with {neurons} Neurons: {corrected_val_loss:.4f}\")\n",
        "\n",
        "                # 最良のハイパーパラメータを更新\n",
        "                if corrected_val_loss < best_loss_overall:\n",
        "                    best_loss_overall = corrected_val_loss\n",
        "                    best_params_overall = {\n",
        "                        'epochs': epochs,\n",
        "                        'batch_size': batch_size,\n",
        "                        'neurons': neurons\n",
        "                    }\n",
        "\n",
        "    # 現在のデータセットでの最良のハイパーパラメータを出力\n",
        "    print(f\"Best Parameters for {filename}: Epochs = {best_params_overall['epochs']}, \"\n",
        "          f\"Batch Size = {best_params_overall['batch_size']}, Neurons = {best_params_overall['neurons']}, \"\n",
        "          f\"Corrected Validation Loss = {best_loss_overall:.4f}\")\n",
        "\n",
        "    # 全体での最良のハイパーパラメータを更新\n",
        "    if best_loss_overall < global_best_loss:\n",
        "        global_best_loss = best_loss_overall\n",
        "        global_best_params = {\n",
        "            'filename': filename,\n",
        "            'epochs': best_params_overall['epochs'],\n",
        "            'batch_size': best_params_overall['batch_size'],\n",
        "            'neurons': best_params_overall['neurons'],\n",
        "            'corrected_val_loss': best_loss_overall\n",
        "        }\n",
        "\n",
        "# 全データセットに対する最良のハイパーパラメータを出力\n",
        "print(f\"\\nGlobal Best Parameters Across All Datasets: Filename = {global_best_params['filename']}, \"\n",
        "      f\"Epochs = {global_best_params['epochs']}, Batch Size = {global_best_params['batch_size']}, \"\n",
        "      f\"Neurons = {global_best_params['neurons']}, Corrected Validation Loss = {global_best_params['corrected_val_loss']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "BTFL5GK_ARcr",
        "outputId": "b3f99c4e-9897-4ee8-fee7-9c84fa89b5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training with 10 epochs, batch size of 32, and 1 neurons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9896e2633a10>:49: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 10 epochs, batch size of 32, and 5 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 10 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 20 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 30 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 40 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 50 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 60 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 70 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 80 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 90 neurons...\n",
            "Training with 10 epochs, batch size of 32, and 100 neurons...\n",
            "Training with 10 epochs, batch size of 64, and 1 neurons...\n",
            "Training with 10 epochs, batch size of 64, and 5 neurons...\n",
            "Training with 10 epochs, batch size of 64, and 10 neurons...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9896e2633a10>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# モデルの訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n\u001b[0m\u001b[1;32m     83\u001b[0m                                     epochs=epochs, batch_size=batch_size, verbose=0)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;31m# Create EpochIterator for evaluation and cache it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_eval_epoch_iterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                     self._eval_epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    386\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_FlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m     34\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_k_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 index_remainder = tf.data.Dataset.from_tensors(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 134\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    733\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0moverload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__tf_tensor__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moverload\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m       ) -> \"Tensor\":\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m       raise ValueError(\n\u001b[1;32m    762\u001b[0m           _add_error_prefix(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     return self._type_enum in (other.as_datatype_enum,\n\u001b[0;32m--> 214\u001b[0;31m                                other.base_dtype.as_datatype_enum)\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mbase_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbase_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"Returns a non-reference `DType` based on this `DType` (for TF1).\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def create_dataset(data, time_step=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/program_LSTM/input-data'\n",
        "\n",
        "# エポックとバッチサイズ、ニューロン数の候補を設定\n",
        "epoch_options = [10, 30, 50, 100]\n",
        "batch_size_options = [32, 64, 128, 256]\n",
        "neuron_options = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "# 各ファイルに対して処理を実行\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(input_folder, filename)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 必要な列のみ抽出\n",
        "    required_columns = ['time', 'Nominal strain', 'Nominal stress']\n",
        "    if not all(column in df.columns for column in required_columns):\n",
        "        print(f\"Required columns not found in {filename}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    df = df[required_columns]\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # レスポンスタイムを1秒ごとにリサンプリングして100点にする\n",
        "    df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
        "\n",
        "    # スケーリング\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # データセット作成\n",
        "    time_step = 10\n",
        "    X, Y = create_dataset(df_scaled, time_step)\n",
        "\n",
        "    # データ分割\n",
        "    train_size = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "    best_loss_overall = float('inf')\n",
        "    best_params_overall = {}\n",
        "\n",
        "    for epochs in epoch_options:\n",
        "        for batch_size in batch_size_options:\n",
        "            best_loss = float('inf')\n",
        "            best_neurons = 0\n",
        "\n",
        "            for neurons in neuron_options:\n",
        "                print(f\"Training with {epochs} epochs, batch size of {batch_size}, and {neurons} neurons...\")\n",
        "\n",
        "                # LSTMモデルを構築\n",
        "                model = Sequential()\n",
        "                model.add(LSTM(neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                model.add(LSTM(neurons))\n",
        "                model.add(Dense(1))\n",
        "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "                # モデルの訓練\n",
        "                history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "                # 検証損失を取得\n",
        "                val_loss = history.history['val_loss'][-1]\n",
        "                print(f\"Validation Loss with {neurons} Neurons: {val_loss:.4f}\")\n",
        "\n",
        "                # 最良のニューロン数を更新\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_neurons = neurons\n",
        "\n",
        "            # パラメータ結果の比較と更新\n",
        "            if best_loss < best_loss_overall:\n",
        "                best_loss_overall = best_loss\n",
        "                best_params_overall = {\n",
        "                    'epochs': epochs,\n",
        "                    'batch_size': batch_size,\n",
        "                    'neurons': best_neurons\n",
        "                }\n",
        "\n",
        "    # 最良のハイパーパラメータを出力\n",
        "    print(f\"Best Parameters for {filename}: Epochs = {best_params_overall['epochs']}, \"\n",
        "          f\"Batch Size = {best_params_overall['batch_size']}, Neurons = {best_params_overall['neurons']}, \"\n",
        "          f\"Validation Loss = {best_loss_overall:.4f}\")"
      ],
      "metadata": {
        "id": "qkFfGKs-BF1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d856ba82-4096-42c7-d47e-849ae80542b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f19e2b5aeb46>:45: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  df = df.resample('1S', on='time').mean().interpolate().head(100)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 10 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.6073\n",
            "Training with 10 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.2018\n",
            "Training with 10 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.2821\n",
            "Training with 10 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0657\n",
            "Training with 10 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0315\n",
            "Training with 10 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0117\n",
            "Training with 10 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0150\n",
            "Training with 10 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0098\n",
            "Training with 10 epochs, batch size of 32, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.0086\n",
            "Training with 10 epochs, batch size of 32, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.0199\n",
            "Training with 10 epochs, batch size of 32, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0239\n",
            "Training with 10 epochs, batch size of 32, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0053\n",
            "Training with 10 epochs, batch size of 64, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 1.0088\n",
            "Training with 10 epochs, batch size of 64, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 1.0638\n",
            "Training with 10 epochs, batch size of 64, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.8336\n",
            "Training with 10 epochs, batch size of 64, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.1454\n",
            "Training with 10 epochs, batch size of 64, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.2371\n",
            "Training with 10 epochs, batch size of 64, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0089\n",
            "Training with 10 epochs, batch size of 64, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0242\n",
            "Training with 10 epochs, batch size of 64, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.1083\n",
            "Training with 10 epochs, batch size of 64, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.1816\n",
            "Training with 10 epochs, batch size of 64, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.2185\n",
            "Training with 10 epochs, batch size of 64, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.2229\n",
            "Training with 10 epochs, batch size of 64, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.1353\n",
            "Training with 10 epochs, batch size of 128, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.6128\n",
            "Training with 10 epochs, batch size of 128, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.4154\n",
            "Training with 10 epochs, batch size of 128, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.4681\n",
            "Training with 10 epochs, batch size of 128, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.2166\n",
            "Training with 10 epochs, batch size of 128, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.3596\n",
            "Training with 10 epochs, batch size of 128, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0038\n",
            "Training with 10 epochs, batch size of 128, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0430\n",
            "Training with 10 epochs, batch size of 128, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0949\n",
            "Training with 10 epochs, batch size of 128, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.1718\n",
            "Training with 10 epochs, batch size of 128, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.1296\n",
            "Training with 10 epochs, batch size of 128, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.1003\n",
            "Training with 10 epochs, batch size of 128, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.0347\n",
            "Training with 10 epochs, batch size of 256, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 0.9287\n",
            "Training with 10 epochs, batch size of 256, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.6520\n",
            "Training with 10 epochs, batch size of 256, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.2365\n",
            "Training with 10 epochs, batch size of 256, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.4475\n",
            "Training with 10 epochs, batch size of 256, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0860\n",
            "Training with 10 epochs, batch size of 256, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0206\n",
            "Training with 10 epochs, batch size of 256, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0700\n",
            "Training with 10 epochs, batch size of 256, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0296\n",
            "Training with 10 epochs, batch size of 256, and 70 neurons...\n",
            "Validation Loss with 70 Neurons: 0.1388\n",
            "Training with 10 epochs, batch size of 256, and 80 neurons...\n",
            "Validation Loss with 80 Neurons: 0.1163\n",
            "Training with 10 epochs, batch size of 256, and 90 neurons...\n",
            "Validation Loss with 90 Neurons: 0.0196\n",
            "Training with 10 epochs, batch size of 256, and 100 neurons...\n",
            "Validation Loss with 100 Neurons: 0.1607\n",
            "Training with 30 epochs, batch size of 32, and 1 neurons...\n",
            "Validation Loss with 1 Neurons: 1.0657\n",
            "Training with 30 epochs, batch size of 32, and 5 neurons...\n",
            "Validation Loss with 5 Neurons: 0.1113\n",
            "Training with 30 epochs, batch size of 32, and 10 neurons...\n",
            "Validation Loss with 10 Neurons: 0.0005\n",
            "Training with 30 epochs, batch size of 32, and 20 neurons...\n",
            "Validation Loss with 20 Neurons: 0.0028\n",
            "Training with 30 epochs, batch size of 32, and 30 neurons...\n",
            "Validation Loss with 30 Neurons: 0.0001\n",
            "Training with 30 epochs, batch size of 32, and 40 neurons...\n",
            "Validation Loss with 40 Neurons: 0.0002\n",
            "Training with 30 epochs, batch size of 32, and 50 neurons...\n",
            "Validation Loss with 50 Neurons: 0.0010\n",
            "Training with 30 epochs, batch size of 32, and 60 neurons...\n",
            "Validation Loss with 60 Neurons: 0.0000\n",
            "Training with 30 epochs, batch size of 32, and 70 neurons...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f19e2b5aeb46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# モデルの訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n\u001b[0m\u001b[1;32m     80\u001b[0m                                     epochs=epochs, batch_size=batch_size, verbose=0)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}